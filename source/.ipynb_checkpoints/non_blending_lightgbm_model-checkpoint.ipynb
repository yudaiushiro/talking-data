{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data... 109903890 149903890\n",
      "loading test data...\n",
      "Extracting new features...\n",
      "selcols ['ip', 'channel'] QQ 4\n",
      "selcols ['ip', 'device', 'os', 'app'] QQ 5\n",
      "selcols ['ip', 'day', 'hour'] QQ 4\n",
      "selcols ['ip', 'app'] QQ 4\n",
      "selcols ['ip', 'app', 'os'] QQ 4\n",
      "selcols ['ip', 'device'] QQ 4\n",
      "selcols ['app', 'channel'] QQ 4\n",
      "selcols ['ip', 'os'] QQ 5\n",
      "selcols ['ip', 'device', 'os', 'app'] QQ 4\n",
      "doing nextClick\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A non-blending lightGBM model that incorporates portions and ideas from various public kernels\n",
    "This kernel gives LB: 0.977 when the parameter 'debug' below is set to 0 but this implementation requires a machine with ~32 GB of memory\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "debug=0\n",
    "if debug:\n",
    "    print('*** debug parameter set: this is a test run for debugging purposes ***')\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.2,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "\n",
    "    return (bst1,bst1.best_iteration)\n",
    "\n",
    "def DO(frm,to,fileno):\n",
    "    dtypes = {\n",
    "            'ip'            : 'uint32',\n",
    "            'app'           : 'uint16',\n",
    "            'device'        : 'uint16',\n",
    "            'os'            : 'uint16',\n",
    "            'channel'       : 'uint16',\n",
    "            'is_attributed' : 'uint8',\n",
    "            'click_id'      : 'uint32',\n",
    "            }\n",
    "\n",
    "    print('loading train data...',frm,to)\n",
    "    train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "    print('loading test data...')\n",
    "    if debug:\n",
    "        test_df = pd.read_csv(\"../input/test.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"../input/test.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "    len_train = len(train_df)\n",
    "    train_df=train_df.append(test_df)\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Extracting new features...')\n",
    "    train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "    train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    naddfeat=9\n",
    "    for i in range(0,naddfeat):\n",
    "        if i==0: selcols=['ip', 'channel']; QQ=4;\n",
    "        if i==1: selcols=['ip', 'device', 'os', 'app']; QQ=5;\n",
    "        if i==2: selcols=['ip', 'day', 'hour']; QQ=4;\n",
    "        if i==3: selcols=['ip', 'app']; QQ=4;\n",
    "        if i==4: selcols=['ip', 'app', 'os']; QQ=4;\n",
    "        if i==5: selcols=['ip', 'device']; QQ=4;\n",
    "        if i==6: selcols=['app', 'channel']; QQ=4;\n",
    "        if i==7: selcols=['ip', 'os']; QQ=5;\n",
    "        if i==8: selcols=['ip', 'device', 'os', 'app']; QQ=4;\n",
    "        print('selcols',selcols,'QQ',QQ)\n",
    "        \n",
    "        filename='X%d_%d_%d.csv'%(i,frm,to)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            if QQ==5: \n",
    "                gp=pd.read_csv(filename,header=None)\n",
    "                train_df['X'+str(i)]=gp\n",
    "            else: \n",
    "                gp=pd.read_csv(filename)\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        else:\n",
    "            if QQ==0:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].count().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==1:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].mean().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==2:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].var().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==3:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].skew().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==4:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].nunique().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==5:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].cumcount()\n",
    "                train_df['X'+str(i)]=gp.values\n",
    "            \n",
    "            if not debug:\n",
    "                 gp.to_csv(filename,index=False)\n",
    "            \n",
    "        del gp\n",
    "        gc.collect()    \n",
    "\n",
    "    print('doing nextClick')\n",
    "    predictors=[]\n",
    "    \n",
    "    new_feature = 'nextClick'\n",
    "    filename='nextClick_%d_%d.csv'%(frm,to)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print('loading from save file')\n",
    "        QQ=pd.read_csv(filename).values\n",
    "    else:\n",
    "        D=2**26\n",
    "        train_df['category'] = (train_df['ip'].astype(str) + \"_\" + train_df['app'].astype(str) + \"_\" + train_df['device'].astype(str) \\\n",
    "            + \"_\" + train_df['os'].astype(str)).apply(hash) % D\n",
    "        click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "        train_df['epochtime']= train_df['click_time'].astype(np.int64) // 10 ** 9\n",
    "        next_clicks= []\n",
    "        for category, t in zip(reversed(train_df['category'].values), reversed(train_df['epochtime'].values)):\n",
    "            next_clicks.append(click_buffer[category]-t)\n",
    "            click_buffer[category]= t\n",
    "        del(click_buffer)\n",
    "        QQ= list(reversed(next_clicks))\n",
    "\n",
    "        if not debug:\n",
    "            print('saving')\n",
    "            pd.DataFrame(QQ).to_csv(filename,index=False)\n",
    "\n",
    "    train_df[new_feature] = QQ\n",
    "    predictors.append(new_feature)\n",
    "\n",
    "    train_df[new_feature+'_shift'] = pd.DataFrame(QQ).shift(+1).values\n",
    "    predictors.append(new_feature+'_shift')\n",
    "    \n",
    "    del QQ\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-day-hour combination...')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app combination...')\n",
    "    gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app-os combination...')\n",
    "    gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    # Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "    print('grouping by : ip_day_chl_var_hour')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_os_var_hour')\n",
    "    gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_channel_var_day')\n",
    "    gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_chl_mean_hour')\n",
    "    gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "    print(\"merging...\")\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"vars and data type: \")\n",
    "    train_df.info()\n",
    "    train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "    train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "    train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "\n",
    "    target = 'is_attributed'\n",
    "    predictors.extend(['app','device','os', 'channel', 'hour', 'day', \n",
    "                  'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "                  'ip_app_os_count', 'ip_app_os_var',\n",
    "                  'ip_app_channel_var_day','ip_app_channel_mean_hour'])\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "    for i in range(0,naddfeat):\n",
    "        predictors.append('X'+str(i))\n",
    "        \n",
    "    print('predictors',predictors)\n",
    "\n",
    "    test_df = train_df[len_train:]\n",
    "    val_df = train_df[(len_train-val_size):len_train]\n",
    "    train_df = train_df[:(len_train-val_size)]\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "    print(\"test size : \", len(test_df))\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.20,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 7,  # 2^max_depth - 1\n",
    "        'max_depth': 3,  # -1 means no limit\n",
    "        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':200 # because training data is extremely unbalanced \n",
    "    }\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                            train_df, \n",
    "                            val_df, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective='binary', \n",
    "                            metrics='auc',\n",
    "                            early_stopping_rounds=30, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=1000, \n",
    "                            categorical_features=categorical)\n",
    "\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "    del train_df\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Plot feature importances...')\n",
    "    ax = lgb.plot_importance(bst, max_num_features=100)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        sub.to_csv('sub_it%d.csv.gz'%(fileno),index=False,compression='gzip')\n",
    "    print(\"done...\")\n",
    "    return sub\n",
    "\n",
    "nrows=184903891-1\n",
    "nchunk=40000000\n",
    "val_size=2500000\n",
    "\n",
    "frm=nrows-75000000\n",
    "if debug:\n",
    "    frm=0\n",
    "    nchunk=100000\n",
    "    val_size=10000\n",
    "\n",
    "to=frm+nchunk\n",
    "\n",
    "sub=DO(frm,to,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
